{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What models or features are you interested in seeing in JetNet? Let us know ! JetNet is a collection of models , datasets , and tools that make it easy to explore neural networks on NVIDIA Jetson (and desktop too!). It can easily be used and extended with Python . It easy to use JetNet comes with tools that allow you to easily build , profile and demo models. This helps you easily try out models to see what is right for your application. For example, here is how you would run a live web demo for different tasks Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: It's implementation agnostic JetNet has well defined interfaces for tasks like classification , detection , pose estimation , and text detection . This means models have a familiar interface, regardless of which framework they are implemented in. As a user, this lets you easily use a variety of models without re-learning a new interface for each one. Classification Detection Pose Text Detection class ClassificationModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError class DetectionModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError class PoseModel : def init ( self ): pass def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError class TextDetectionModel : def init ( self ): pass def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError It's highly reproducible and configurable JetNet models are defined as pydantic types, which means they they can be easily validated, modified, and exported to JSON. The models include an init function which is used to perform all steps necessary to prepare the model for execution, like downloading weights, downloading calibration data and optimizing with TensorRT. For example, the following models, which include TensorRT optimization can be re-created with a single line Classification Detection Pose Text Detection from jetnet.torchvision import RESNET18_IMAGENET_TRT_FP16 model = RESNET18_IMAGENET_TRT_FP16 . build () from jetnet.yolox import YOLOX_NANO_TRT_FP16 model = YOLOX_NANO_TRT_FP16 . build () from jetnet.trt_pose import RESNET18_BODY_224X224_TRT_FP16 model = RESNET18_BODY_224X224_TRT_FP16 . build () from jetnet.easyocr import EASYOCR_EN_TRT_FP16 model = EASYOCR_EN_TRT_FP16 . build () It's easy to set up JetNet comes with pre-built docker containers for Jetson and Desktop. In case these don't work for you, manual setup instructions are provided. Check out the Setup page for details. It's extensible JetNet is written with Python so that it is easy to extend. If you want to use the JetNet tools with a different model, or are considering contributing to the project to help other developers easily use your model, all you need to do is implement one of the JetNet interfaces . For example, here's how we might define a new classification model Definition ( cat_dog.py ) from pydantic import PrivateAttr class CatDogModel ( ClassificationModel ): num_layers : int # private attributes can be non-JSON types, like a PyTorch module _torch_module = PrivateAttr () def init ( self ): # code to initialize model for execution def get_labels ( self ) -> Sequence [ str ]: return [ \"cat\" , \"dog\" ] def __call__ ( self , x : Image ) -> Classification : # code to classify image CATDOG_SMALL = CatDogModel ( num_layers = 10 ) CATDOG_BIG = CatDogModel ( num_layers = 50 ) We can then use the model with JetNet tools. jetnet demo cat_dog.CATDOG_SMALL Get Started! Head on over the Setup to configure your system to run JetNet. Please note, if a task isn't supported that you would like to see in JetNet, let us know on GitHub. You can open an issue, discussion or even a pull-request to get things started. We welcome all feedback!","title":"Home"},{"location":"#it-easy-to-use","text":"JetNet comes with tools that allow you to easily build , profile and demo models. This helps you easily try out models to see what is right for your application. For example, here is how you would run a live web demo for different tasks Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections:","title":"It easy to use"},{"location":"#its-implementation-agnostic","text":"JetNet has well defined interfaces for tasks like classification , detection , pose estimation , and text detection . This means models have a familiar interface, regardless of which framework they are implemented in. As a user, this lets you easily use a variety of models without re-learning a new interface for each one. Classification Detection Pose Text Detection class ClassificationModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError class DetectionModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError class PoseModel : def init ( self ): pass def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError class TextDetectionModel : def init ( self ): pass def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError","title":"It's implementation agnostic"},{"location":"#its-highly-reproducible-and-configurable","text":"JetNet models are defined as pydantic types, which means they they can be easily validated, modified, and exported to JSON. The models include an init function which is used to perform all steps necessary to prepare the model for execution, like downloading weights, downloading calibration data and optimizing with TensorRT. For example, the following models, which include TensorRT optimization can be re-created with a single line Classification Detection Pose Text Detection from jetnet.torchvision import RESNET18_IMAGENET_TRT_FP16 model = RESNET18_IMAGENET_TRT_FP16 . build () from jetnet.yolox import YOLOX_NANO_TRT_FP16 model = YOLOX_NANO_TRT_FP16 . build () from jetnet.trt_pose import RESNET18_BODY_224X224_TRT_FP16 model = RESNET18_BODY_224X224_TRT_FP16 . build () from jetnet.easyocr import EASYOCR_EN_TRT_FP16 model = EASYOCR_EN_TRT_FP16 . build ()","title":"It's highly reproducible and configurable"},{"location":"#its-easy-to-set-up","text":"JetNet comes with pre-built docker containers for Jetson and Desktop. In case these don't work for you, manual setup instructions are provided. Check out the Setup page for details.","title":"It's easy to set up"},{"location":"#its-extensible","text":"JetNet is written with Python so that it is easy to extend. If you want to use the JetNet tools with a different model, or are considering contributing to the project to help other developers easily use your model, all you need to do is implement one of the JetNet interfaces . For example, here's how we might define a new classification model Definition ( cat_dog.py ) from pydantic import PrivateAttr class CatDogModel ( ClassificationModel ): num_layers : int # private attributes can be non-JSON types, like a PyTorch module _torch_module = PrivateAttr () def init ( self ): # code to initialize model for execution def get_labels ( self ) -> Sequence [ str ]: return [ \"cat\" , \"dog\" ] def __call__ ( self , x : Image ) -> Classification : # code to classify image CATDOG_SMALL = CatDogModel ( num_layers = 10 ) CATDOG_BIG = CatDogModel ( num_layers = 50 ) We can then use the model with JetNet tools. jetnet demo cat_dog.CATDOG_SMALL","title":"It's extensible"},{"location":"#get-started","text":"Head on over the Setup to configure your system to run JetNet. Please note, if a task isn't supported that you would like to see in JetNet, let us know on GitHub. You can open an issue, discussion or even a pull-request to get things started. We welcome all feedback!","title":"Get Started!"},{"location":"datasets/","text":"This page contains pre-defined dataset configs. Copy the dataset name to use it with the JetNet tools. Image Datasets Class Name Config RemoteImageFolder jetnet.coco.COCO2017_VAL_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TRAIN_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TRAIN_IMAGES json","title":"Datasets"},{"location":"datasets/#image-datasets","text":"Class Name Config RemoteImageFolder jetnet.coco.COCO2017_VAL_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TRAIN_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TRAIN_IMAGES json","title":"Image Datasets"},{"location":"models/","text":"This page contains pre-defined model configs. Copy the model name to use it with the JetNet tools. Classification Models Class Name Config TorchvisionModel jetnet.torchvision.RESNET18_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET34_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET50_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET101_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET152_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET121_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET161_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET169_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET201_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.MOBILENET_V2_IMAGENET json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_INT8 json Detection Models Class Name Config YOLOX jetnet.yolox.YOLOX_L json YOLOXTRT jetnet.yolox.YOLOX_L_TRT json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_M json YOLOXTRT jetnet.yolox.YOLOX_M_TRT json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_S json YOLOXTRT jetnet.yolox.YOLOX_S_TRT json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_X json YOLOXTRT jetnet.yolox.YOLOX_X_TRT json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_TINY json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_NANO json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_INT8 json Pose Models Class Name Config TRTPose jetnet.trt_pose.RESNET18_BODY_224X224 json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT_INT8 json TRTPose jetnet.trt_pose.DENSENET121_BODY_256X256 json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_INT8 json TRTPose jetnet.trt_pose.RESNET18_HAND_224X224 json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT_INT8 json Text Detection Models Class Name Config EasyOCR jetnet.easyocr.EASYOCR_EN json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_FP16 json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_CRNN json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_INT8_FP16 json","title":"Models"},{"location":"models/#classification-models","text":"Class Name Config TorchvisionModel jetnet.torchvision.RESNET18_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET34_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET50_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET101_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET152_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET121_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET161_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET169_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET201_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.MOBILENET_V2_IMAGENET json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_INT8 json","title":"Classification Models"},{"location":"models/#detection-models","text":"Class Name Config YOLOX jetnet.yolox.YOLOX_L json YOLOXTRT jetnet.yolox.YOLOX_L_TRT json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_M json YOLOXTRT jetnet.yolox.YOLOX_M_TRT json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_S json YOLOXTRT jetnet.yolox.YOLOX_S_TRT json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_X json YOLOXTRT jetnet.yolox.YOLOX_X_TRT json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_TINY json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_NANO json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_INT8 json","title":"Detection Models"},{"location":"models/#pose-models","text":"Class Name Config TRTPose jetnet.trt_pose.RESNET18_BODY_224X224 json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.RESNET18_BODY_224X224_TRT_INT8 json TRTPose jetnet.trt_pose.DENSENET121_BODY_256X256 json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_INT8 json TRTPose jetnet.trt_pose.RESNET18_HAND_224X224 json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 json TRTPoseTRT jetnet.trt_pose.RESNET18_HAND_224X224_TRT_INT8 json","title":"Pose Models"},{"location":"models/#text-detection-models","text":"Class Name Config EasyOCR jetnet.easyocr.EASYOCR_EN json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_FP16 json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_CRNN json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_INT8_FP16 json","title":"Text Detection Models"},{"location":"setup/","text":"This page details setup steps needed to start using JetNet Docker Setup JetNet comes with pre-built docker containers for some system configurations. If you have the disk space and there is an available container, this is a fast and easy option for getting started. To use the container, first clone the github repo git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet Next, launch the docker container from inside the cloned directory Jetson (JetPack 5.0.1) Desktop (NV driver 465.19.01+) docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-34.1.1 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:x86-21.05 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" This will mount the current directory (which should be the jetnet project root) at /jetnet inside the container. Most data downloaded when using JetNet is stored in the data folder. So assuming you use JetNet command line tools from /jetnet inside the container, the data will persist upon container restart. Note, this command assumes you have a USB camera at /dev/video0. Please adjust the command accordingly. Building docker containers You may want to build the containers yourself, if you have additional dependencies, or need to use a different base container. Below are the commands we use to build the pre-made containers. Check the GitHub repo docker files for more details. docker build -t jaybdub/jetnet:l4t-34.1.1 -f $( pwd ) /docker/l4t-34.1.1/Dockerfile $( pwd ) /docker/l4t-34.1.1 Manual Setup If there is not a container available for your platform, or you don't have the storage space, you can set up your system natively. Install TensorRT, PyTorch, OpenCV and Torchvision (please refer to external instructions) Install miscellanerous dependencies pip3 install pydantic progressbar python3-socketio uvicorn starlette Install torch2trt pip3 install git+https://github.com/NVIDIA-AI-IOT/torch2trt.git@master Install YOLOX (required for jetnet.yolox ) git clone https://github.com/Megvii-BaseDetection/YOLOX cd YOLOX python3 setup.py install cd .. Install EasyOCR (required for jetnet.easyocr ) pip3 install git+https://github.com/JaidedAI/EasyOCR.git@v1.5.0 Install TRTPose (required for jetnet.trt_pose ) pip3 install git+https://github.com/NVIDIA-AI-IOT/trt_pose.git Install JetNet git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet python3 setup.py develop Currently we exclude jetnet.mmocr from manual setup. For now, please reference the dockerfile in the GitHub repo if you wish to use these models.","title":"Setup"},{"location":"setup/#docker-setup","text":"JetNet comes with pre-built docker containers for some system configurations. If you have the disk space and there is an available container, this is a fast and easy option for getting started. To use the container, first clone the github repo git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet Next, launch the docker container from inside the cloned directory Jetson (JetPack 5.0.1) Desktop (NV driver 465.19.01+) docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-34.1.1 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:x86-21.05 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" This will mount the current directory (which should be the jetnet project root) at /jetnet inside the container. Most data downloaded when using JetNet is stored in the data folder. So assuming you use JetNet command line tools from /jetnet inside the container, the data will persist upon container restart. Note, this command assumes you have a USB camera at /dev/video0. Please adjust the command accordingly. Building docker containers You may want to build the containers yourself, if you have additional dependencies, or need to use a different base container. Below are the commands we use to build the pre-made containers. Check the GitHub repo docker files for more details. docker build -t jaybdub/jetnet:l4t-34.1.1 -f $( pwd ) /docker/l4t-34.1.1/Dockerfile $( pwd ) /docker/l4t-34.1.1","title":"Docker Setup"},{"location":"setup/#manual-setup","text":"If there is not a container available for your platform, or you don't have the storage space, you can set up your system natively. Install TensorRT, PyTorch, OpenCV and Torchvision (please refer to external instructions) Install miscellanerous dependencies pip3 install pydantic progressbar python3-socketio uvicorn starlette Install torch2trt pip3 install git+https://github.com/NVIDIA-AI-IOT/torch2trt.git@master Install YOLOX (required for jetnet.yolox ) git clone https://github.com/Megvii-BaseDetection/YOLOX cd YOLOX python3 setup.py install cd .. Install EasyOCR (required for jetnet.easyocr ) pip3 install git+https://github.com/JaidedAI/EasyOCR.git@v1.5.0 Install TRTPose (required for jetnet.trt_pose ) pip3 install git+https://github.com/NVIDIA-AI-IOT/trt_pose.git Install JetNet git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet python3 setup.py develop Currently we exclude jetnet.mmocr from manual setup. For now, please reference the dockerfile in the GitHub repo if you wish to use these models.","title":"Manual Setup"},{"location":"tools/","text":"Pick a pre-defined model and use it with these tools. For this example, we'll use the jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 model. You can also define your our own model or dataset and use it with these tools as long as it can be imported in Python. Build jetnet build is a convenience tool that simply imports the model and calls model.build() . This is useful for testing if a model builds and for generating cached data before using the model elsewhere. To use it, call jetnet build <model> . For example, Classification Detection Pose Text Detection jetnet build jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet build jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet build jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet build jetnet.easyocr.EASYOCR_EN_TRT_FP16 Profile jetnet profile profiles a model on real data. It measures the model throughput, as well as other task specific statistics like the average number of objects per image. This is handy, especially for models that may have data-dependent runtime. To use it, call jetnet profile <model> <dataset> . For example, Classification Detection Pose Text Detection jetnet profile jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 159.50995530176425 , \"avg_image_area\" : 286686.08 } jetnet profile jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 161.64499986521764 , \"avg_image_area\" : 286686.08 , \"avg_num_detections\" : 3.86 } jetnet profile jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 103.2494480449782 , \"avg_image_area\" : 286686.08 , \"avg_num_poses\" : 1.98 , \"avg_num_keypoints\" : 16.32 } jetnet profile jetnet.easyocr.EASYOCR_EN_TRT_FP16 jetnet.textocr.TEXTOCR_TEST_IMAGES example output: { \"fps\" : 13.334012937781655 , \"avg_image_area\" : 768962.56 , \"avg_num_detections\" : 10.48 , \"avg_num_characters\" : 66.46 } Demo jetnet demo peforms inference on live camera images and displays predictions in your web browser. This is especially handy when you're operating on a headless machine. With a USB camera attached, call jetnet demo <model> . For example, Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions.","title":"Tools"},{"location":"tools/#build","text":"jetnet build is a convenience tool that simply imports the model and calls model.build() . This is useful for testing if a model builds and for generating cached data before using the model elsewhere. To use it, call jetnet build <model> . For example, Classification Detection Pose Text Detection jetnet build jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet build jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet build jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet build jetnet.easyocr.EASYOCR_EN_TRT_FP16","title":"Build"},{"location":"tools/#profile","text":"jetnet profile profiles a model on real data. It measures the model throughput, as well as other task specific statistics like the average number of objects per image. This is handy, especially for models that may have data-dependent runtime. To use it, call jetnet profile <model> <dataset> . For example, Classification Detection Pose Text Detection jetnet profile jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 159.50995530176425 , \"avg_image_area\" : 286686.08 } jetnet profile jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 161.64499986521764 , \"avg_image_area\" : 286686.08 , \"avg_num_detections\" : 3.86 } jetnet profile jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 103.2494480449782 , \"avg_image_area\" : 286686.08 , \"avg_num_poses\" : 1.98 , \"avg_num_keypoints\" : 16.32 } jetnet profile jetnet.easyocr.EASYOCR_EN_TRT_FP16 jetnet.textocr.TEXTOCR_TEST_IMAGES example output: { \"fps\" : 13.334012937781655 , \"avg_image_area\" : 768962.56 , \"avg_num_detections\" : 10.48 , \"avg_num_characters\" : 66.46 }","title":"Profile"},{"location":"tools/#demo","text":"jetnet demo peforms inference on live camera images and displays predictions in your web browser. This is especially handy when you're operating on a headless machine. With a USB camera attached, call jetnet demo <model> . For example, Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions.","title":"Demo"},{"location":"python/reference/","text":"Common Types Point class Point : x : int y : int Polygon class Polygon : points : Sequence [ Point ] Classification class Classification : index : int label : Optional [ str ] score : Optional [ float ] Detection class Detection : boundary : Polygon classification : Classification DetectionSet class DetectionSet : detections : Sequence [ Detection ] Keypoint class Keypoint : x : int y : int index : int label : Optional [ str ] score : Optional [ float ] Pose class Pose : keypoints : Sequence [ Keypoint ] PoseSet class PoseSet : poses : Sequence [ Pose ] TextDetection class TextDetection : boundary : Polygon text : Optional [ str ] score : Optional [ float ] TextDetectionSet class TextDetectionSet : detections : Sequence [ Detection ] Abstract Types ClassificationModel class ClassificationModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError DetectionModel class DetectionModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError PoseModel class PoseModel : def init ( self ): pass def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError TextDetectionModel class TextDetectionModel : def init ( self ): pass def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError ImageDataset class ImageDataset : def init ( self ): pass def __len__ ( self ) -> int : raise NotImplementedError def __getitem__ ( self , index : int ) -> Image : raise NotImplementedError","title":"Reference"},{"location":"python/reference/#common-types","text":"","title":"Common Types"},{"location":"python/reference/#point","text":"class Point : x : int y : int","title":"Point"},{"location":"python/reference/#polygon","text":"class Polygon : points : Sequence [ Point ]","title":"Polygon"},{"location":"python/reference/#classification","text":"class Classification : index : int label : Optional [ str ] score : Optional [ float ]","title":"Classification"},{"location":"python/reference/#detection","text":"class Detection : boundary : Polygon classification : Classification","title":"Detection"},{"location":"python/reference/#detectionset","text":"class DetectionSet : detections : Sequence [ Detection ]","title":"DetectionSet"},{"location":"python/reference/#keypoint","text":"class Keypoint : x : int y : int index : int label : Optional [ str ] score : Optional [ float ]","title":"Keypoint"},{"location":"python/reference/#pose","text":"class Pose : keypoints : Sequence [ Keypoint ]","title":"Pose"},{"location":"python/reference/#poseset","text":"class PoseSet : poses : Sequence [ Pose ]","title":"PoseSet"},{"location":"python/reference/#textdetection","text":"class TextDetection : boundary : Polygon text : Optional [ str ] score : Optional [ float ]","title":"TextDetection"},{"location":"python/reference/#textdetectionset","text":"class TextDetectionSet : detections : Sequence [ Detection ]","title":"TextDetectionSet"},{"location":"python/reference/#abstract-types","text":"","title":"Abstract Types"},{"location":"python/reference/#classificationmodel","text":"class ClassificationModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError","title":"ClassificationModel"},{"location":"python/reference/#detectionmodel","text":"class DetectionModel : def init ( self ): pass def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError","title":"DetectionModel"},{"location":"python/reference/#posemodel","text":"class PoseModel : def init ( self ): pass def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError","title":"PoseModel"},{"location":"python/reference/#textdetectionmodel","text":"class TextDetectionModel : def init ( self ): pass def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError","title":"TextDetectionModel"},{"location":"python/reference/#imagedataset","text":"class ImageDataset : def init ( self ): pass def __len__ ( self ) -> int : raise NotImplementedError def __getitem__ ( self , index : int ) -> Image : raise NotImplementedError","title":"ImageDataset"},{"location":"python/usage/","text":"Model usage Build a model To build a model, call the following from jetnet.yolox import YOLOX_TINY_TRT_FP16 model = YOLOX_TINY_TRT_FP16 . build () Perform inference Once the model is built, you can then perform inference import PIL.Image image = PIL . Image . open ( \"assets/person.jpg\" ) output = model ( image ) print ( output . json ( indent = 2 )) Output { \"detections\" : [ { \"boundary\" : { \"points\" : [ { \"x\" : 312 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 1304 }, { \"x\" : 312 , \"y\" : 1304 } ] }, \"classification\" : { \"index\" : 0 , \"label\" : \"person\" , \"score\" : 0.9122651219367981 } } ] } Customize a model You can customize the model by copying and modifying it before building model = YOLOX_TINY_TRT_FP16 . copy ( deep = True ) model . model . input_size = ( 1280 , 736 ) model . engine_cache = \"data/custom_model.pth\" model = model . build () Dump a model to JSON All models are JSON serializable, so we can view the model like this print ( model . json ( indent = 2 )) Output { \"model\" : { \"exp\" : \"yolox_tiny\" , \"input_size\" : [ 1280 , 736 ], \"labels\" : [ \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"backpack\" , \"umbrella\" , \"handbag\" , \"tie\" , \"suitcase\" , \"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" , \"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" , \"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" , \"dining table\" , \"toilet\" , \"tv\" , \"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" , \"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"book\" , \"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" ], \"conf_thresh\" : 0.3 , \"nms_thresh\" : 0.3 , \"device\" : \"cuda\" , \"weights_path\" : \"data/yolox/yolox_tiny.pth\" , \"weights_url\" : \"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth\" }, \"int8_mode\" : false , \"fp16_mode\" : true , \"max_workspace_size\" : 33554432 , \"engine_cache\" : \"data/custom_model.pth\" , \"int8_calib_dataset\" : { \"image_folder\" : { \"path\" : \"data/coco/val2017\" , \"recursive\" : false }, \"zip_url\" : \"http://images.cocodataset.org/zips/val2017.zip\" , \"zip_folder\" : \"val2017\" , \"zip_file\" : \"data/coco/val2017.zip\" }, \"int8_calib_cache\" : \"data/yolox/yolox_tiny_calib\" , \"int8_num_calib\" : 512 , \"int8_calib_algorithm\" : \"entropy_2\" } Define a custom model class You can create your own model by subclassing the related base model for the task. For example, from pydantic import PrivateAttr class CatDogModel ( ClassificationModel ): num_layers : int # private attributes can be non-JSON types, like a PyTorch module _torch_module = PrivateAttr () def init ( self ): # code to initialize model for execution def get_labels ( self ) -> Sequence [ str ]: return [ \"cat\" , \"dog\" ] def __call__ ( self , x : Image ) -> Classification : # code to classify image You can then define some instances of your model CAT_DOG_SMALL = CatDogModel ( num_layers = 10 ) CAT_DOG_BIG = CatDogModel ( num_layers = 50 ) If the model can be imported in Python, it can be used with the command line tools. Suppose we have our models defined in ./cat_dog.py , we could use a model like this jetnet demo cat_dog . CAT_DOG_SMALL Dataset usage Build a dataset First, you build the dataset like this from jetnet.coco import COCO2017_VAL_IMAGES dataset = COCO2017_VAL_IMAGES . build () Get the size Once the dataset is built, you can determine the size of the dataset using the len method like this size = len ( dataset ) Read a sample To read a sample from the dataset, do this image = dataset [ 0 ] Use your own images If you have images in a folder, you can create a dataset for them like this CAT_DOG_IMAGES = ImageFolder ( path = \"images\" ) Assuming this is defined in ./cat_dog.py you could then use it with the command line tools like this jetnet profile cat_dog . CAT_DOG_SMALL cat_dog . CAT_DOG_IMAGES It's worth checking out the RemoteImageFolder, so you can store your images remotely, and automatically download it. This will make your dataset more reproducible. Define a custom dataset class If the pre-made dataset classes don't fit your use case, you can create your own dataset class like this from jetnet.image import ImageDataset class CatDogImages ( ImageDataset ): def init ( self ): # code to prepare dataset for reading, ie: downloading data def __len__ ( self ) -> int : # code to get length of dataset def __getitem__ ( self ) -> Image : # code to read sample from dataset","title":"Usage"},{"location":"python/usage/#model-usage","text":"","title":"Model usage"},{"location":"python/usage/#build-a-model","text":"To build a model, call the following from jetnet.yolox import YOLOX_TINY_TRT_FP16 model = YOLOX_TINY_TRT_FP16 . build ()","title":"Build a model"},{"location":"python/usage/#perform-inference","text":"Once the model is built, you can then perform inference import PIL.Image image = PIL . Image . open ( \"assets/person.jpg\" ) output = model ( image ) print ( output . json ( indent = 2 )) Output { \"detections\" : [ { \"boundary\" : { \"points\" : [ { \"x\" : 312 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 1304 }, { \"x\" : 312 , \"y\" : 1304 } ] }, \"classification\" : { \"index\" : 0 , \"label\" : \"person\" , \"score\" : 0.9122651219367981 } } ] }","title":"Perform inference"},{"location":"python/usage/#customize-a-model","text":"You can customize the model by copying and modifying it before building model = YOLOX_TINY_TRT_FP16 . copy ( deep = True ) model . model . input_size = ( 1280 , 736 ) model . engine_cache = \"data/custom_model.pth\" model = model . build ()","title":"Customize a model"},{"location":"python/usage/#dump-a-model-to-json","text":"All models are JSON serializable, so we can view the model like this print ( model . json ( indent = 2 )) Output { \"model\" : { \"exp\" : \"yolox_tiny\" , \"input_size\" : [ 1280 , 736 ], \"labels\" : [ \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"backpack\" , \"umbrella\" , \"handbag\" , \"tie\" , \"suitcase\" , \"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" , \"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" , \"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" , \"dining table\" , \"toilet\" , \"tv\" , \"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" , \"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"book\" , \"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" ], \"conf_thresh\" : 0.3 , \"nms_thresh\" : 0.3 , \"device\" : \"cuda\" , \"weights_path\" : \"data/yolox/yolox_tiny.pth\" , \"weights_url\" : \"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth\" }, \"int8_mode\" : false , \"fp16_mode\" : true , \"max_workspace_size\" : 33554432 , \"engine_cache\" : \"data/custom_model.pth\" , \"int8_calib_dataset\" : { \"image_folder\" : { \"path\" : \"data/coco/val2017\" , \"recursive\" : false }, \"zip_url\" : \"http://images.cocodataset.org/zips/val2017.zip\" , \"zip_folder\" : \"val2017\" , \"zip_file\" : \"data/coco/val2017.zip\" }, \"int8_calib_cache\" : \"data/yolox/yolox_tiny_calib\" , \"int8_num_calib\" : 512 , \"int8_calib_algorithm\" : \"entropy_2\" }","title":"Dump a model to JSON"},{"location":"python/usage/#define-a-custom-model-class","text":"You can create your own model by subclassing the related base model for the task. For example, from pydantic import PrivateAttr class CatDogModel ( ClassificationModel ): num_layers : int # private attributes can be non-JSON types, like a PyTorch module _torch_module = PrivateAttr () def init ( self ): # code to initialize model for execution def get_labels ( self ) -> Sequence [ str ]: return [ \"cat\" , \"dog\" ] def __call__ ( self , x : Image ) -> Classification : # code to classify image You can then define some instances of your model CAT_DOG_SMALL = CatDogModel ( num_layers = 10 ) CAT_DOG_BIG = CatDogModel ( num_layers = 50 ) If the model can be imported in Python, it can be used with the command line tools. Suppose we have our models defined in ./cat_dog.py , we could use a model like this jetnet demo cat_dog . CAT_DOG_SMALL","title":"Define a custom model class"},{"location":"python/usage/#dataset-usage","text":"","title":"Dataset usage"},{"location":"python/usage/#build-a-dataset","text":"First, you build the dataset like this from jetnet.coco import COCO2017_VAL_IMAGES dataset = COCO2017_VAL_IMAGES . build ()","title":"Build a dataset"},{"location":"python/usage/#get-the-size","text":"Once the dataset is built, you can determine the size of the dataset using the len method like this size = len ( dataset )","title":"Get the size"},{"location":"python/usage/#read-a-sample","text":"To read a sample from the dataset, do this image = dataset [ 0 ]","title":"Read a sample"},{"location":"python/usage/#use-your-own-images","text":"If you have images in a folder, you can create a dataset for them like this CAT_DOG_IMAGES = ImageFolder ( path = \"images\" ) Assuming this is defined in ./cat_dog.py you could then use it with the command line tools like this jetnet profile cat_dog . CAT_DOG_SMALL cat_dog . CAT_DOG_IMAGES It's worth checking out the RemoteImageFolder, so you can store your images remotely, and automatically download it. This will make your dataset more reproducible.","title":"Use your own images"},{"location":"python/usage/#define-a-custom-dataset-class","text":"If the pre-made dataset classes don't fit your use case, you can create your own dataset class like this from jetnet.image import ImageDataset class CatDogImages ( ImageDataset ): def init ( self ): # code to prepare dataset for reading, ie: downloading data def __len__ ( self ) -> int : # code to get length of dataset def __getitem__ ( self ) -> Image : # code to read sample from dataset","title":"Define a custom dataset class"}]}