{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What models or features are you interested in seeing in JetNet? Let us know ! JetNet is a collection of models , datasets , and tools that make it easy to explore neural networks on NVIDIA Jetson (and desktop too!). It can easily be used and extended with Python . It easy to use JetNet comes with tools that allow you to easily build , profile and demo models. This helps you easily try out models to see what is right for your application. For example, here is how you would run a live web demo for different tasks Classification Detection Pose Text Detection Instance Segmentation jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: It's implementation agnostic JetNet has well defined interfaces for tasks like classification , detection , pose estimation , and text detection . This means models have a familiar interface, regardless of which framework they are implemented in. As a user, this lets you easily use a variety of models without re-learning a new interface for each one. Classification Detection Pose Text Detection class ClassificationModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError class DetectionModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError class PoseModel : def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError class TextDetectionModel : def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError It's highly reproducible and configurable JetNet models are defined as pydantic types, which means they they can be easily validated, modified, and exported to JSON. The models include an init function which is used to perform all steps necessary to prepare the model for execution, like downloading weights, downloading calibration data and optimizing with TensorRT. For example, the following models, which include TensorRT optimization can be re-created with a single line Classification Detection Pose Text Detection Instance Segmentation from jetnet.torchvision import RESNET18_IMAGENET_TRT_FP16 model = RESNET18_IMAGENET_TRT_FP16 . build () from jetnet.yolox import YOLOX_NANO_TRT_FP16 model = YOLOX_NANO_TRT_FP16 . build () from jetnet.trt_pose import RESNET18_BODY_224X224_TRT_FP16 model = RESNET18_BODY_224X224_TRT_FP16 . build () from jetnet.easyocr import EASYOCR_EN_TRT_FP16 model = EASYOCR_EN_TRT_FP16 . build () from jetnet.mmdet import MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 model = MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 . build () It's easy to set up JetNet comes with pre-built docker containers for Jetson and Desktop. In case these don't work for you, manual setup instructions are provided. Check out the Setup page for details. Get Started! Head on over the Setup to configure your system to run JetNet. Please note, if a task isn't supported that you would like to see in JetNet, let us know on GitHub. You can open an issue, discussion or even a pull-request to get things started. We welcome all feedback!","title":"Home"},{"location":"#it-easy-to-use","text":"JetNet comes with tools that allow you to easily build , profile and demo models. This helps you easily try out models to see what is right for your application. For example, here is how you would run a live web demo for different tasks Classification Detection Pose Text Detection Instance Segmentation jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections: jetnet demo jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 and then open your browser to <ip_address>:8000 to view the detections:","title":"It easy to use"},{"location":"#its-implementation-agnostic","text":"JetNet has well defined interfaces for tasks like classification , detection , pose estimation , and text detection . This means models have a familiar interface, regardless of which framework they are implemented in. As a user, this lets you easily use a variety of models without re-learning a new interface for each one. Classification Detection Pose Text Detection class ClassificationModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError class DetectionModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError class PoseModel : def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError class TextDetectionModel : def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError","title":"It's implementation agnostic"},{"location":"#its-highly-reproducible-and-configurable","text":"JetNet models are defined as pydantic types, which means they they can be easily validated, modified, and exported to JSON. The models include an init function which is used to perform all steps necessary to prepare the model for execution, like downloading weights, downloading calibration data and optimizing with TensorRT. For example, the following models, which include TensorRT optimization can be re-created with a single line Classification Detection Pose Text Detection Instance Segmentation from jetnet.torchvision import RESNET18_IMAGENET_TRT_FP16 model = RESNET18_IMAGENET_TRT_FP16 . build () from jetnet.yolox import YOLOX_NANO_TRT_FP16 model = YOLOX_NANO_TRT_FP16 . build () from jetnet.trt_pose import RESNET18_BODY_224X224_TRT_FP16 model = RESNET18_BODY_224X224_TRT_FP16 . build () from jetnet.easyocr import EASYOCR_EN_TRT_FP16 model = EASYOCR_EN_TRT_FP16 . build () from jetnet.mmdet import MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 model = MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 . build ()","title":"It's highly reproducible and configurable"},{"location":"#its-easy-to-set-up","text":"JetNet comes with pre-built docker containers for Jetson and Desktop. In case these don't work for you, manual setup instructions are provided. Check out the Setup page for details.","title":"It's easy to set up"},{"location":"#get-started","text":"Head on over the Setup to configure your system to run JetNet. Please note, if a task isn't supported that you would like to see in JetNet, let us know on GitHub. You can open an issue, discussion or even a pull-request to get things started. We welcome all feedback!","title":"Get Started!"},{"location":"datasets/","text":"This page contains pre-defined dataset configs. Copy the dataset name to use it with the JetNet tools. Image Datasets Class Name Config RemoteImageFolder jetnet.coco.COCO2017_VAL_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TRAIN_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TRAIN_IMAGES json","title":"Datasets"},{"location":"datasets/#image-datasets","text":"Class Name Config RemoteImageFolder jetnet.coco.COCO2017_VAL_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TRAIN_IMAGES json RemoteImageFolder jetnet.coco.COCO2017_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TEST_IMAGES json RemoteImageFolder jetnet.textocr.TEXTOCR_TRAIN_IMAGES json","title":"Image Datasets"},{"location":"models/","text":"This page contains pre-defined model configs. Copy the model name to use it with the JetNet tools. Classification Models Class Name Config TorchvisionModel jetnet.torchvision.RESNET18_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET34_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET50_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET101_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET152_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET121_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET161_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET169_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET201_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.MOBILENET_V2_IMAGENET json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_INT8 json Detection Models Class Name Config YOLOX jetnet.yolox.YOLOX_L json YOLOXTRT jetnet.yolox.YOLOX_L_TRT json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_M json YOLOXTRT jetnet.yolox.YOLOX_M_TRT json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_S json YOLOXTRT jetnet.yolox.YOLOX_S_TRT json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_X json YOLOXTRT jetnet.yolox.YOLOX_X_TRT json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_TINY json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_NANO json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_INT8 json MMDet jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT json Instance Segmentation Models Class Name Config MMDet jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT json Pose Models Class Name Config TRTPose jetnet.trt_pose.RESNET18_BODY_224X224 json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT_INT8 json TRTPose jetnet.trt_pose.DENSENET121_BODY_256X256 json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_FP16 json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_INT8 json TRTPose jetnet.trt_pose.RESNET18_HAND_224X224 json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT_INT8 json Text Detection Models Class Name Config EasyOCR jetnet.easyocr.EASYOCR_EN json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_FP16 json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_CRNN json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_INT8_FP16 json","title":"Models"},{"location":"models/#classification-models","text":"Class Name Config TorchvisionModel jetnet.torchvision.RESNET18_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET18_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET34_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET34_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET50_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET50_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET101_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET101_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.RESNET152_IMAGENET json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.RESNET152_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET121_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET121_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET161_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET161_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET169_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET169_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.DENSENET201_IMAGENET json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.DENSENET201_IMAGENET_TRT_INT8 json TorchvisionModel jetnet.torchvision.MOBILENET_V2_IMAGENET json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_FP16 json TorchvisionModelTRT jetnet.torchvision.MOBILENET_V2_IMAGENET_TRT_INT8 json","title":"Classification Models"},{"location":"models/#detection-models","text":"Class Name Config YOLOX jetnet.yolox.YOLOX_L json YOLOXTRT jetnet.yolox.YOLOX_L_TRT json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_L_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_M json YOLOXTRT jetnet.yolox.YOLOX_M_TRT json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_M_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_S json YOLOXTRT jetnet.yolox.YOLOX_S_TRT json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_S_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_X json YOLOXTRT jetnet.yolox.YOLOX_X_TRT json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_X_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_TINY json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_TINY_TRT_INT8 json YOLOX jetnet.yolox.YOLOX_NANO json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_FP16 json YOLOXTRT jetnet.yolox.YOLOX_NANO_TRT_INT8 json MMDet jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT json","title":"Detection Models"},{"location":"models/#instance-segmentation-models","text":"Class Name Config MMDet jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT_FP16 json Torch2trtModel jetnet.mmdet.MASK_RCNN_R50_FPN_1X_COCO_TRT json","title":"Instance Segmentation Models"},{"location":"models/#pose-models","text":"Class Name Config TRTPose jetnet.trt_pose.RESNET18_BODY_224X224 json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 json Torch2trtModel jetnet.trt_pose.RESNET18_BODY_224X224_TRT_INT8 json TRTPose jetnet.trt_pose.DENSENET121_BODY_256X256 json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_FP16 json Torch2trtModel jetnet.trt_pose.DENSENET121_BODY_256X256_TRT_INT8 json TRTPose jetnet.trt_pose.RESNET18_HAND_224X224 json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT_FP16 json Torch2trtModel jetnet.trt_pose.RESNET18_HAND_224X224_TRT_INT8 json","title":"Pose Models"},{"location":"models/#text-detection-models","text":"Class Name Config EasyOCR jetnet.easyocr.EASYOCR_EN json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_FP16 json EasyOCRTRT jetnet.easyocr.EASYOCR_EN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_CRNN json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_CRNN_TRT_INT8_FP16 json MMOCR jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_FP16 json MMOCRTRT jetnet.mmocr.MMOCR_DB_R18_ROBUSTSCANNER_TRT_INT8_FP16 json","title":"Text Detection Models"},{"location":"setup/","text":"This page details setup steps needed to start using JetNet Docker Setup JetNet comes with pre-built docker containers for some system configurations. If you have the disk space and there is an available container, this is a fast and easy option for getting started. To use the container, first clone the github repo git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet Next, launch the docker container from inside the cloned directory Jetson (JetPack 5.0.2) Jetson (JetPack 5.0.1) Desktop (NV driver 465.19.01+) docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-34.1.1 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:x86-21.05 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" This will mount the current directory (which should be the jetnet project root) at /jetnet inside the container. Most data downloaded when using JetNet is stored in the data folder. So assuming you use JetNet command line tools from /jetnet inside the container, the data will persist upon container restart. Note, this command assumes you have a USB camera at /dev/video0. Please adjust the command accordingly. Building docker containers You may want to build the containers yourself, if you have additional dependencies, or need to use a different base container. Below are the commands we use to build the pre-made containers. Check the GitHub repo docker files for more details. docker build -t jaybdub/jetnet:l4t-34.1.1 -f $( pwd ) /docker/l4t-34.1.1/Dockerfile $( pwd ) /docker/l4t-34.1.1 Manual Setup If there is not a container available for your platform, or you don't have the storage space, you can set up your system natively. Install TensorRT, PyTorch, OpenCV and Torchvision (please refer to external instructions) Install miscellanerous dependencies pip3 install pydantic progressbar python3-socketio uvicorn starlette Install torch2trt pip3 install git+https://github.com/NVIDIA-AI-IOT/torch2trt.git@master Install YOLOX (required for jetnet.yolox ) git clone https://github.com/Megvii-BaseDetection/YOLOX cd YOLOX python3 setup.py install cd .. Install EasyOCR (required for jetnet.easyocr ) pip3 install git+https://github.com/JaidedAI/EasyOCR.git@v1.5.0 Install TRTPose (required for jetnet.trt_pose ) pip3 install git+https://github.com/NVIDIA-AI-IOT/trt_pose.git Install JetNet git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet python3 setup.py develop Currently we exclude jetnet.mmocr from manual setup. For now, please reference the dockerfile in the GitHub repo if you wish to use these models.","title":"Setup"},{"location":"setup/#docker-setup","text":"JetNet comes with pre-built docker containers for some system configurations. If you have the disk space and there is an available container, this is a fast and easy option for getting started. To use the container, first clone the github repo git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet Next, launch the docker container from inside the cloned directory Jetson (JetPack 5.0.2) Jetson (JetPack 5.0.1) Desktop (NV driver 465.19.01+) docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-34.1.1 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" docker run \\ --network host \\ --gpus all \\ -it \\ --rm \\ --name = jetnet \\ -v $( pwd ) :/jetnet \\ --device /dev/video0 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:x86-21.05 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && /bin/bash\" This will mount the current directory (which should be the jetnet project root) at /jetnet inside the container. Most data downloaded when using JetNet is stored in the data folder. So assuming you use JetNet command line tools from /jetnet inside the container, the data will persist upon container restart. Note, this command assumes you have a USB camera at /dev/video0. Please adjust the command accordingly. Building docker containers You may want to build the containers yourself, if you have additional dependencies, or need to use a different base container. Below are the commands we use to build the pre-made containers. Check the GitHub repo docker files for more details. docker build -t jaybdub/jetnet:l4t-34.1.1 -f $( pwd ) /docker/l4t-34.1.1/Dockerfile $( pwd ) /docker/l4t-34.1.1","title":"Docker Setup"},{"location":"setup/#manual-setup","text":"If there is not a container available for your platform, or you don't have the storage space, you can set up your system natively. Install TensorRT, PyTorch, OpenCV and Torchvision (please refer to external instructions) Install miscellanerous dependencies pip3 install pydantic progressbar python3-socketio uvicorn starlette Install torch2trt pip3 install git+https://github.com/NVIDIA-AI-IOT/torch2trt.git@master Install YOLOX (required for jetnet.yolox ) git clone https://github.com/Megvii-BaseDetection/YOLOX cd YOLOX python3 setup.py install cd .. Install EasyOCR (required for jetnet.easyocr ) pip3 install git+https://github.com/JaidedAI/EasyOCR.git@v1.5.0 Install TRTPose (required for jetnet.trt_pose ) pip3 install git+https://github.com/NVIDIA-AI-IOT/trt_pose.git Install JetNet git clone https://github.com/NVIDIA-AI-IOT/jetnet cd jetnet python3 setup.py develop Currently we exclude jetnet.mmocr from manual setup. For now, please reference the dockerfile in the GitHub repo if you wish to use these models.","title":"Manual Setup"},{"location":"tools/","text":"Pick a pre-defined model and use it with these tools. For this example, we'll use the jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 model. You can also define your our own model or dataset and use it with these tools as long as it can be imported in Python. Build jetnet build is a convenience tool that simply imports the model and calls model.build() . This is useful for testing if a model builds and for generating cached data before using the model elsewhere. To use it, call jetnet build <model> . For example, Classification Detection Pose Text Detection jetnet build jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet build jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet build jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet build jetnet.easyocr.EASYOCR_EN_TRT_FP16 Profile jetnet profile profiles a model on real data. It measures the model throughput, as well as other task specific statistics like the average number of objects per image. This is handy, especially for models that may have data-dependent runtime. To use it, call jetnet profile <model> <dataset> . For example, Classification Detection Pose Text Detection jetnet profile jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 159.50995530176425 , \"avg_image_area\" : 286686.08 } jetnet profile jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 161.64499986521764 , \"avg_image_area\" : 286686.08 , \"avg_num_detections\" : 3.86 } jetnet profile jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 103.2494480449782 , \"avg_image_area\" : 286686.08 , \"avg_num_poses\" : 1.98 , \"avg_num_keypoints\" : 16.32 } jetnet profile jetnet.easyocr.EASYOCR_EN_TRT_FP16 jetnet.textocr.TEXTOCR_TEST_IMAGES example output: { \"fps\" : 13.334012937781655 , \"avg_image_area\" : 768962.56 , \"avg_num_detections\" : 10.48 , \"avg_num_characters\" : 66.46 } Demo jetnet demo peforms inference on live camera images and displays predictions in your web browser. This is especially handy when you're operating on a headless machine. With a USB camera attached, call jetnet demo <model> . For example, Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions.","title":"Tools"},{"location":"tools/#build","text":"jetnet build is a convenience tool that simply imports the model and calls model.build() . This is useful for testing if a model builds and for generating cached data before using the model elsewhere. To use it, call jetnet build <model> . For example, Classification Detection Pose Text Detection jetnet build jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet build jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet build jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet build jetnet.easyocr.EASYOCR_EN_TRT_FP16","title":"Build"},{"location":"tools/#profile","text":"jetnet profile profiles a model on real data. It measures the model throughput, as well as other task specific statistics like the average number of objects per image. This is handy, especially for models that may have data-dependent runtime. To use it, call jetnet profile <model> <dataset> . For example, Classification Detection Pose Text Detection jetnet profile jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 159.50995530176425 , \"avg_image_area\" : 286686.08 } jetnet profile jetnet.yolox.YOLOX_NANO_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 161.64499986521764 , \"avg_image_area\" : 286686.08 , \"avg_num_detections\" : 3.86 } jetnet profile jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 jetnet.coco.COCO2017_VAL_IMAGES example output: { \"fps\" : 103.2494480449782 , \"avg_image_area\" : 286686.08 , \"avg_num_poses\" : 1.98 , \"avg_num_keypoints\" : 16.32 } jetnet profile jetnet.easyocr.EASYOCR_EN_TRT_FP16 jetnet.textocr.TEXTOCR_TEST_IMAGES example output: { \"fps\" : 13.334012937781655 , \"avg_image_area\" : 768962.56 , \"avg_num_detections\" : 10.48 , \"avg_num_characters\" : 66.46 }","title":"Profile"},{"location":"tools/#demo","text":"jetnet demo peforms inference on live camera images and displays predictions in your web browser. This is especially handy when you're operating on a headless machine. With a USB camera attached, call jetnet demo <model> . For example, Classification Detection Pose Text Detection jetnet demo jetnet.torchvision.RESNET18_IMAGENET_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.yolox.YOLOX_NANO_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.trt_pose.RESNET18_BODY_224X224_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions. jetnet demo jetnet.easyocr.EASYOCR_EN_TRT_FP16 Once the demo is running, navigate to http://<ip>:8000 in your web browser to view the predictions.","title":"Demo"},{"location":"python/reference/","text":"Common Types Point class Point : x : int y : int Polygon class Polygon : points : Sequence [ Point ] Classification class Classification : index : int label : Optional [ str ] score : Optional [ float ] Detection class Detection : boundary : Polygon classification : Optional [ Classification ] mask : Optional [ BinaryMask ] DetectionSet class DetectionSet : detections : Sequence [ Detection ] Keypoint class Keypoint : x : int y : int index : int label : Optional [ str ] score : Optional [ float ] Pose class Pose : keypoints : Sequence [ Keypoint ] PoseSet class PoseSet : poses : Sequence [ Pose ] TextDetection class TextDetection : boundary : Polygon text : Optional [ str ] score : Optional [ float ] TextDetectionSet class TextDetectionSet : detections : Sequence [ Detection ] Abstract Types ClassificationModel class ClassificationModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError DetectionModel class DetectionModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError PoseModel class PoseModel : def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError TextDetectionModel class TextDetectionModel : def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError ImageDataset class ImageDataset : def __len__ ( self ) -> int : raise NotImplementedError def __getitem__ ( self , index : int ) -> Image : raise NotImplementedError","title":"Reference"},{"location":"python/reference/#common-types","text":"","title":"Common Types"},{"location":"python/reference/#point","text":"class Point : x : int y : int","title":"Point"},{"location":"python/reference/#polygon","text":"class Polygon : points : Sequence [ Point ]","title":"Polygon"},{"location":"python/reference/#classification","text":"class Classification : index : int label : Optional [ str ] score : Optional [ float ]","title":"Classification"},{"location":"python/reference/#detection","text":"class Detection : boundary : Polygon classification : Optional [ Classification ] mask : Optional [ BinaryMask ]","title":"Detection"},{"location":"python/reference/#detectionset","text":"class DetectionSet : detections : Sequence [ Detection ]","title":"DetectionSet"},{"location":"python/reference/#keypoint","text":"class Keypoint : x : int y : int index : int label : Optional [ str ] score : Optional [ float ]","title":"Keypoint"},{"location":"python/reference/#pose","text":"class Pose : keypoints : Sequence [ Keypoint ]","title":"Pose"},{"location":"python/reference/#poseset","text":"class PoseSet : poses : Sequence [ Pose ]","title":"PoseSet"},{"location":"python/reference/#textdetection","text":"class TextDetection : boundary : Polygon text : Optional [ str ] score : Optional [ float ]","title":"TextDetection"},{"location":"python/reference/#textdetectionset","text":"class TextDetectionSet : detections : Sequence [ Detection ]","title":"TextDetectionSet"},{"location":"python/reference/#abstract-types","text":"","title":"Abstract Types"},{"location":"python/reference/#classificationmodel","text":"class ClassificationModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> Classification : raise NotImplementedError","title":"ClassificationModel"},{"location":"python/reference/#detectionmodel","text":"class DetectionModel : def get_labels ( self ) -> Sequence [ str ]: raise NotImplementedError def __call__ ( self , x : Image ) -> DetectionSet : raise NotImplementedError","title":"DetectionModel"},{"location":"python/reference/#posemodel","text":"class PoseModel : def get_keypoints ( self ) -> Sequence [ str ]: raise NotImplementedError def get_skeleton ( self ) -> Sequence [ Tuple [ int , int ]]: raise NotImplementedError def __call__ ( self , x : Image ) -> PoseSet : raise NotImplementedError","title":"PoseModel"},{"location":"python/reference/#textdetectionmodel","text":"class TextDetectionModel : def __call__ ( self , x : Image ) -> TextDetectionSet : raise NotImplementedError","title":"TextDetectionModel"},{"location":"python/reference/#imagedataset","text":"class ImageDataset : def __len__ ( self ) -> int : raise NotImplementedError def __getitem__ ( self , index : int ) -> Image : raise NotImplementedError","title":"ImageDataset"},{"location":"python/usage/","text":"Model usage Build a model To build a model, call the following from jetnet.yolox import YOLOX_TINY_TRT_FP16 model = YOLOX_TINY_TRT_FP16 . build () Perform inference Once the model is built, you can then perform inference import PIL.Image image = PIL . Image . open ( \"assets/person.jpg\" ) output = model ( image ) print ( output . json ( indent = 2 )) Output { \"detections\" : [ { \"boundary\" : { \"points\" : [ { \"x\" : 312 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 1304 }, { \"x\" : 312 , \"y\" : 1304 } ] }, \"classification\" : { \"index\" : 0 , \"label\" : \"person\" , \"score\" : 0.9122651219367981 } } ] } Customize a model You can customize the model by copying and modifying it before building model = YOLOX_TINY_TRT_FP16 . copy ( deep = True ) model . model . input_size = ( 1280 , 736 ) model . engine_cache = \"data/custom_model.pth\" model = model . build () Dump a model to JSON All models are JSON serializable, so we can view the model like this print ( model . json ( indent = 2 )) Output { \"model\" : { \"exp\" : \"yolox_tiny\" , \"input_size\" : [ 1280 , 736 ], \"labels\" : [ \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"backpack\" , \"umbrella\" , \"handbag\" , \"tie\" , \"suitcase\" , \"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" , \"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" , \"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" , \"dining table\" , \"toilet\" , \"tv\" , \"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" , \"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"book\" , \"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" ], \"conf_thresh\" : 0.3 , \"nms_thresh\" : 0.3 , \"device\" : \"cuda\" , \"weights_path\" : \"data/yolox/yolox_tiny.pth\" , \"weights_url\" : \"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth\" }, \"int8_mode\" : false , \"fp16_mode\" : true , \"max_workspace_size\" : 33554432 , \"engine_cache\" : \"data/custom_model.pth\" , \"int8_calib_dataset\" : { \"image_folder\" : { \"path\" : \"data/coco/val2017\" , \"recursive\" : false }, \"zip_url\" : \"http://images.cocodataset.org/zips/val2017.zip\" , \"zip_folder\" : \"val2017\" , \"zip_file\" : \"data/coco/val2017.zip\" }, \"int8_calib_cache\" : \"data/yolox/yolox_tiny_calib\" , \"int8_num_calib\" : 512 , \"int8_calib_algorithm\" : \"entropy_2\" } Dataset usage Build a dataset First, you build the dataset like this from jetnet.coco import COCO2017_VAL_IMAGES dataset = COCO2017_VAL_IMAGES . build () Get the size Once the dataset is built, you can determine the size of the dataset using the len method like this size = len ( dataset ) Read a sample To read a sample from the dataset, do this image = dataset [ 0 ]","title":"Usage"},{"location":"python/usage/#model-usage","text":"","title":"Model usage"},{"location":"python/usage/#build-a-model","text":"To build a model, call the following from jetnet.yolox import YOLOX_TINY_TRT_FP16 model = YOLOX_TINY_TRT_FP16 . build ()","title":"Build a model"},{"location":"python/usage/#perform-inference","text":"Once the model is built, you can then perform inference import PIL.Image image = PIL . Image . open ( \"assets/person.jpg\" ) output = model ( image ) print ( output . json ( indent = 2 )) Output { \"detections\" : [ { \"boundary\" : { \"points\" : [ { \"x\" : 312 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 262 }, { \"x\" : 667 , \"y\" : 1304 }, { \"x\" : 312 , \"y\" : 1304 } ] }, \"classification\" : { \"index\" : 0 , \"label\" : \"person\" , \"score\" : 0.9122651219367981 } } ] }","title":"Perform inference"},{"location":"python/usage/#customize-a-model","text":"You can customize the model by copying and modifying it before building model = YOLOX_TINY_TRT_FP16 . copy ( deep = True ) model . model . input_size = ( 1280 , 736 ) model . engine_cache = \"data/custom_model.pth\" model = model . build ()","title":"Customize a model"},{"location":"python/usage/#dump-a-model-to-json","text":"All models are JSON serializable, so we can view the model like this print ( model . json ( indent = 2 )) Output { \"model\" : { \"exp\" : \"yolox_tiny\" , \"input_size\" : [ 1280 , 736 ], \"labels\" : [ \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"backpack\" , \"umbrella\" , \"handbag\" , \"tie\" , \"suitcase\" , \"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" , \"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" , \"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" , \"dining table\" , \"toilet\" , \"tv\" , \"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" , \"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"book\" , \"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" ], \"conf_thresh\" : 0.3 , \"nms_thresh\" : 0.3 , \"device\" : \"cuda\" , \"weights_path\" : \"data/yolox/yolox_tiny.pth\" , \"weights_url\" : \"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth\" }, \"int8_mode\" : false , \"fp16_mode\" : true , \"max_workspace_size\" : 33554432 , \"engine_cache\" : \"data/custom_model.pth\" , \"int8_calib_dataset\" : { \"image_folder\" : { \"path\" : \"data/coco/val2017\" , \"recursive\" : false }, \"zip_url\" : \"http://images.cocodataset.org/zips/val2017.zip\" , \"zip_folder\" : \"val2017\" , \"zip_file\" : \"data/coco/val2017.zip\" }, \"int8_calib_cache\" : \"data/yolox/yolox_tiny_calib\" , \"int8_num_calib\" : 512 , \"int8_calib_algorithm\" : \"entropy_2\" }","title":"Dump a model to JSON"},{"location":"python/usage/#dataset-usage","text":"","title":"Dataset usage"},{"location":"python/usage/#build-a-dataset","text":"First, you build the dataset like this from jetnet.coco import COCO2017_VAL_IMAGES dataset = COCO2017_VAL_IMAGES . build ()","title":"Build a dataset"},{"location":"python/usage/#get-the-size","text":"Once the dataset is built, you can determine the size of the dataset using the len method like this size = len ( dataset )","title":"Get the size"},{"location":"python/usage/#read-a-sample","text":"To read a sample from the dataset, do this image = dataset [ 0 ]","title":"Read a sample"},{"location":"tutorials/multiple_containers/","text":"This tutorial details how to run multiple JetNet containers asynchronously with a single webcam as a shared input. Step 1 - Setup a V4L2 loopback device (virtual camera) To share a webcam across multiple containers, set up a v4l2loopback device. First, install some dependencies sudo apt-get update sudo apt-get install v4l2loopback-dkms v4l-utils ffmpeg Check which camera devices are currently on the system. ls /dev/video* v4l2-ctl --list-devices Create a v4l2loopback device with an unused device ID. We'll assume device ID 10 is unused. sudo modprobe v4l2loopback video_nr = 10 exclusive_caps = 1 card_label = \"Virtual webcam\" You should find /dev/video10 got created. You can verify this again by calling v4l2-ctl --list-devices . Step 2 - Link the real camera to the loopback device In one terminal, use ffmpeg to stream the real camera device (which we'll assume is /dev/video0 ) to the virtual camera we created /dev/video10 ffmpeg -f v4l2 -i /dev/video0 -f v4l2 /dev/video10 This will keep running, so leave this terminal open. Step 3 - Run an object detection container on the loopback camera In a new terminal, and launch the first container for object detection. We'll assume the working directory is the root of the cloned jetnet repository. sudo docker run \\ --network host \\ --gpus all \\ --runtime nvidia \\ -it \\ --rm \\ --name = jetnet1 \\ -v $( pwd ) :/jetnet \\ --device /dev/video10 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && jetnet demo --port 8080 --camera_device 10 jetnet.yolox.YOLOX_NANO_TRT_FP16\" Open a web browser and access http://<IP_ADDRESS>:8080 . If you are using the same Jetson to run the web browser, it is http://0.0.0.0:8080 Step 4 - Run a text detection container on the original camera In another new terminal, and launch the second container for text detection. Notice that we use a different port. sudo docker run \\ --network host \\ --gpus all \\ --runtime nvidia \\ -it \\ --rm \\ --name = jetnet2 \\ -v $( pwd ) :/jetnet \\ --device /dev/video10 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && jetnet demo --port 8081 --camera_device 10 jetnet.easyocr.EASYOCR_EN_TRT_FP16\" Open a web browser and access http://<IP_ADDRESS>:8081 . If you are using the same Jetson to run the web browser, it is http://0.0.0.0:8081 Result Once both containers are open, you should be able to view the outputs in separate browser tabs like this. That's all for this tutorial! If you run into any issues, feel free to open an issue on GitHub.","title":"Run multiple containers at once"},{"location":"tutorials/multiple_containers/#step-1-setup-a-v4l2-loopback-device-virtual-camera","text":"To share a webcam across multiple containers, set up a v4l2loopback device. First, install some dependencies sudo apt-get update sudo apt-get install v4l2loopback-dkms v4l-utils ffmpeg Check which camera devices are currently on the system. ls /dev/video* v4l2-ctl --list-devices Create a v4l2loopback device with an unused device ID. We'll assume device ID 10 is unused. sudo modprobe v4l2loopback video_nr = 10 exclusive_caps = 1 card_label = \"Virtual webcam\" You should find /dev/video10 got created. You can verify this again by calling v4l2-ctl --list-devices .","title":"Step 1 - Setup a V4L2 loopback device (virtual camera)"},{"location":"tutorials/multiple_containers/#step-2-link-the-real-camera-to-the-loopback-device","text":"In one terminal, use ffmpeg to stream the real camera device (which we'll assume is /dev/video0 ) to the virtual camera we created /dev/video10 ffmpeg -f v4l2 -i /dev/video0 -f v4l2 /dev/video10 This will keep running, so leave this terminal open.","title":"Step 2 - Link the real camera to the loopback device"},{"location":"tutorials/multiple_containers/#step-3-run-an-object-detection-container-on-the-loopback-camera","text":"In a new terminal, and launch the first container for object detection. We'll assume the working directory is the root of the cloned jetnet repository. sudo docker run \\ --network host \\ --gpus all \\ --runtime nvidia \\ -it \\ --rm \\ --name = jetnet1 \\ -v $( pwd ) :/jetnet \\ --device /dev/video10 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && jetnet demo --port 8080 --camera_device 10 jetnet.yolox.YOLOX_NANO_TRT_FP16\" Open a web browser and access http://<IP_ADDRESS>:8080 . If you are using the same Jetson to run the web browser, it is http://0.0.0.0:8080","title":"Step 3 - Run an object detection container on the loopback camera"},{"location":"tutorials/multiple_containers/#step-4-run-a-text-detection-container-on-the-original-camera","text":"In another new terminal, and launch the second container for text detection. Notice that we use a different port. sudo docker run \\ --network host \\ --gpus all \\ --runtime nvidia \\ -it \\ --rm \\ --name = jetnet2 \\ -v $( pwd ) :/jetnet \\ --device /dev/video10 \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = $DISPLAY \\ jaybdub/jetnet:l4t-35.1.0 \\ /bin/bash -c \"cd /jetnet && python3 setup.py develop && jetnet demo --port 8081 --camera_device 10 jetnet.easyocr.EASYOCR_EN_TRT_FP16\" Open a web browser and access http://<IP_ADDRESS>:8081 . If you are using the same Jetson to run the web browser, it is http://0.0.0.0:8081","title":"Step 4 - Run a text detection container on the original camera"},{"location":"tutorials/multiple_containers/#result","text":"Once both containers are open, you should be able to view the outputs in separate browser tabs like this. That's all for this tutorial! If you run into any issues, feel free to open an issue on GitHub.","title":"Result"}]}